setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)
# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv
dados<- read.csv("train_sample.csv")
head(dados)
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
# Limpeza e manipulação
# Verificando a existência de NAs no arquivo
variaveis <- names(dados)
for(v in variaveis){
print('-------------')
print(paste("Número de NAs na variável", v))
print(sum(is.na(dados[v])))
}
str(dados)
dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)
dados %>%
group_by(is_attributed) %>%
summarise(Vazio = sum(attributed_time == ''),
Alguma_coisa = sum(attributed_time != ''))
dados$attributed_time <- NULL
rm(dados_filt)
# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.
# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second.
# A variável original click_time será eliminada do dataset.
dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL
t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)
t_last - t_1st
# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.
dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)
# Inspeção gráfica de relações entre os atributos e o label.
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x)) +
facet_grid(dados$is_attributed ~ .)})
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)
# Inspeção gráfica após o balanceamento
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria após balanceamento")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x,"após balanceamento")) +
facet_grid(dados$is_attributed ~ .)})
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
dados_cat <- dados
variaveis <- names(dados_cat)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
dados_cat[[v]] <- cut(dados_cat[[v]], 53)
}
#dados_cat <- dados
#variaveis <- names(dados)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
str(dados)
dados_cat <- NULL
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "hour"]
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
dados_cat <- NULL
variaveis <- names(dados)
variaveis <- variaveis[!(variaveis in c("is_attributed", "hour"))]
variaveis <- variaveis[!(variaveis %in% c("is_attributed", "hour"))]
variaveis
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
str(dados_cat)
str(dados)
dados_cat <- dados
variaveis <- names(dados)
variaveis <- variaveis[!(variaveis %in% c("is_attributed", "hour"))]
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
# Categorizando também o atributo hour, que tem menos que 53 níveis
dados_cat$hour <- factor(dados_cat$hour)
str(dados_cat)
str(dados)
##################################################################
#### Projeto 1 - Detecção de fraudes - TalkingData AdTracking ####
##################################################################
setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-1/")
getwd()
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(data.table)
library(e1071)
# Utilizando para testes e desenvolvimento na máquina local o arquivo train_sample.csv
# por motivos de menor utilização de memória da máquina. Na versão para produção, será utilizado
# o arquivo completo train.csv
dados<- read.csv("train_sample.csv")
head(dados)
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
# Limpeza e manipulação
# Verificando a existência de NAs no arquivo
variaveis <- names(dados)
for(v in variaveis){
print('-------------')
print(paste("Número de NAs na variável", v))
print(sum(is.na(dados[v])))
}
str(dados)
dados_filt <- dados %>% filter(attributed_time != '' & is_attributed != 1)
head(dados_filt)
dados %>%
group_by(is_attributed) %>%
summarise(Vazio = sum(attributed_time == ''),
Alguma_coisa = sum(attributed_time != ''))
dados$attributed_time <- NULL
rm(dados_filt)
# Tratando a variável click_time: pode-se avaliar o tempo tanto como uma variável contínua,
# no sentido de se analisar uma tendência de aumento do número de fraudes ao longo do tempo
# como também de forma categorizada (por mês, dia do mês, dia da semana, hora do dia, etc.)
# para analisar possíveis sazonalidades no número de fraudes ao longo do ano.
# Para isto será criada uma variável click_time_posixct, que é a conversão da variável
# click_time para o formato POSIXct. Poderão ser criadas variáveis como rel_click_time,
# que será o tempo relativo entre o menor tempo do dataset e cada tempo registrado,
# e também variáveis categóricas como year, month, week, weekday, hour, minute, second.
# A variável original click_time será eliminada do dataset.
dados$click_time_posixct <- as.POSIXct(dados$click_time)
dados$click_time <- NULL
t_1st <- min(dados$click_time_posixct)
t_last <- max(dados$click_time_posixct)
t_last - t_1st
# Como o tempo decorrido entre o primeiro e o último clique é de apenas 3 dias, não parece
# fazer sentido um aumento do número de fraudes significativo ao longo desse curto intervalo de
# tempo. De modo que as variáveis  year, month, week, weekday não serão criadas para poupar
# esforço computacional. Serão criadas, então, apenas as variáveis hour, minute, second.
# Também para reduzir esforço computacional, será eliminada do dataset de treino a variável
# click_time_posixct.
dados$hour <- hour(dados$click_time_posixct)
dados$minute <- minute(dados$click_time_posixct)
dados$second <- second(dados$click_time_posixct)
dados$click_time_posixct <- NULL
str(dados)
summary(dados)
# Inspeção gráfica de relações entre os atributos e o label.
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x)) +
facet_grid(dados$is_attributed ~ .)})
# Balanceando o dataset
# Criando um arquivo CSV para carregar no Azure ML
write.csv(dados, "dados_para_balancear.csv", row.names = FALSE)
# Lendo um arquivo CSV retornado pelo Azure ML
dados <- read.csv("dados_balanceados.csv")
str(dados)
# Retornando o label para o tipo fator
dados$is_attributed <- factor(dados$is_attributed)
# Inspeção gráfica após o balanceamento
ggplot(dados, aes(is_attributed)) + geom_bar(fill = "blue", alpha = 0.5) +
ggtitle("Totais de is_attributed por categoria após balanceamento")
table(dados$is_attributed)
variaveis <- names(dados)
variaveis <- variaveis[variaveis != "is_attributed"]
lapply(variaveis, function(x){
ggplot(dados, aes_string(x)) +
geom_histogram(fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de",x,"após balanceamento")) +
facet_grid(dados$is_attributed ~ .)})
## Criando um dataset categorizado para posterior teste dos modelos
variaveis <- names(dados)
dados_cat <- dados
for(v in variaveis){
dados_cat[[v]] <- factor(dados_cat[[v]])
}
str(dados_cat)
dados_cat <- dados
variaveis <- names(dados)
variaveis <- variaveis[!(variaveis %in% c("is_attributed", "hour"))]
for(v in variaveis){
dados_cat[[v]] <- cut(dados[[v]], 53)
}
# Categorizando também o atributo hour, que tem menos que 53 níveis
dados_cat$hour <- factor(dados_cat$hour)
str(dados_cat)
str(dados)
# Fazendo o split data entre dados de treino e dados de validação, uma vez que os dados
# de teste não tem label disponível.
split <- createDataPartition(y = dados$is_attributed, p = 0.7, list = FALSE)
dados_treino <- dados[split, ]
dados_valid <- dados[-split, ]
dados_cat_treino <- dados_cat[split, ]
dados_cat_valid <- dados_cat[-split, ]
###################################################
#### Primeiro modelo de Machine Learning ##########
###################################################
# Algoritmo: randomForest
# Atributos: não-categorizados
modelo_rf1 <- randomForest(is_attributed ~ .,
data = dados_treino,
ntree = 100, nodesize = 10, importance = T)
previsao_rf1 <- predict(modelo_rf1, dados_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_rf1, dados_valid$is_attributed)
###################################################
#### Segundo modelo de Machine Learning ###########
###################################################
# Algoritmo: randomForest
# Atributos: categorizados
modelo_rf2 <- randomForest(is_attributed ~ .,
data = dados_cat_treino,
ntree = 100, nodesize = 10, importance = T)
previsao_rf2 <- predict(modelo_rf2, dados_cat_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_rf2, dados_cat_valid$is_attributed)
###################################################
#### Terceiro modelo de Machine Learning ##########
###################################################
# Algoritmo: SVM
# Atributos: não-categorizados
modelo_svm1 <- svm(is_attributed ~ .,
data = dados_treino,
type = 'C-classification',
kernel = 'radial')
previsao_svm1 <- predict(modelo_svm1, dados_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
###################################################
#### Quarto modelo de Machine Learning ############
###################################################
# Algoritmo: SVM
# Atributos: categorizados
modelo_svm2 <- svm(is_attributed ~ .,
data = dados_cat_treino,
type = 'C-classification',
kernel = 'radial')
previsao_svm2 <- predict(modelo_svm2, dados_cat_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_svm2, dados_cat_valid$is_attributed)
###################################################
#### Quinto modelo de Machine Learning ############
###################################################
# Algoritmo: Naive-Bayes
# Atributos: não-categorizados
modelo_nb1 <- naiveBayes(dados[, -6], dados[, 6])
previsao_nb1 <- predict(modelo_nb1, dados_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
###################################################
#### Sexto modelo de Machine Learning #############
###################################################
# Algoritmo: Naive-Bayes
# Atributos: categorizados
modelo_nb2 <- naiveBayes(dados_cat[, -6], dados[, 6])
previsao_nb2 <- predict(modelo_nb2, dados_cat_valid, type = 'class')
# Matriz de confusão
confusionMatrix(previsao_svm1, dados_valid$is_attributed)
###################################################
#### Predição do arquivo test.csv #################
###################################################
# Melhor modelo: modelo_rf1
test <- as.data.frame(fread("test.csv"))
submission <- as.data.frame(fread("sample_submission.csv"))
head(test)
head(submission)
str(test)
str(submission)
# Para adequar o dataset de teste ao dataset de treinamento do modelo 1
# é necessário excluir a variável click_id e formatar a variável click_time
test$click_id <- NULL
test$click_time_posixct <- as.POSIXct(test$click_time)
test$click_time <- NULL
test$hour <- hour(test$click_time_posixct)
test$minute <- minute(test$click_time_posixct)
test$second <- second(test$click_time_posixct)
test$click_time_posixct <- NULL
str(test)
previsao_final <- predict(modelo_rf1, test)
table(previsao_final)
# Gravando o arquivo submission para envio para o Kaggle
submission$is_attributed <- previsao_final
fwrite(submission, "submission.csv")
##################################################################
######### Projeto 2 - Previsão de demanda - Grupo Bimbo ##########
##################################################################
Azure <- FALSE
if(!Azure){
setwd("C:/DataScience/FCD/BigDataAnalytics-R-Azure/Projeto-2/previsao_demanda_grupo_bimbo/")
getwd()
}
############################################################
#### Carregamento de dados e bibliotecas ###################
############################################################
if(Azure){
data <- maml.mapInputPort(1) # class: data.frame
test <- maml.mapInputPort(2) # class: data.frame
}else{
library(readr)
data <- read_csv("dados/train_sample_v2.csv")
test <- read_csv("dados/test.csv")
#cliente_tabla <- read_csv("dados/cliente_tabla.csv")
#producto_tabla <- read_csv("dados/producto_tabla.csv")
#town_state <- read_csv("dados/town_state.csv")
#sample_submission <- read_csv("dados/sample_submission.csv")
}
##############################################
#### Análise Exploratória dos Dados ##########
##############################################
head(data)
head(test)
# Únicas variáveis preditoras no test set:
# Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID
#str(data)
summary(data)
summary(test)
# Verificando dados faltantes nos datasets
conta_NA <- function(X){
return(sum(is.na(X)))
}
lapply(data, conta_NA)
lapply(test, conta_NA)
# As variáveis Venta_uni_hoy Venta_hoy Dev_uni_proxima Dev_proxima não se encontram
# presentes no dataset de treinamento, portanto serão excluídas.
if(!Azure){
data$Venta_hoy <- NULL
data$Venta_uni_hoy <- NULL
data$Dev_proxima <- NULL
data$Dev_uni_proxima <- NULL
}
# Normalização dos dados do dataset data
min_data <- lapply(data, min)
max_data <- lapply(data, max)
min_test <- lapply(test, min)
max_test <- lapply(test, max)
normalizar <- function(x, min, max){
return((x-min)/(max-min))
}
for(v in names(data)){
if(v %in% names(test)){
min <- min(min_data[[v]], min_test[[v]])
max <- max(max_data[[v]], max_test[[v]])
}else{
min <- min_data[[v]]
max <- max_data[[v]]
}
data[[v]] <- normalizar(data[[v]], min, max)
}
# Normalização do dataset test
# Não será feita a normalização do index, porque o index não é variável preditora.
for(v in names(test)[-1]){
if(v %in% names(data)){
min <- min(min_data[[v]], min_test[[v]])
max <- max(max_data[[v]], max_test[[v]])
}else{
min <- min_test[[v]]
max <- max_test[[v]]
}
test[[v]] <- normalizar(test[[v]], min, max)
}
################ Correlação ##########################
# Métodos de Correlação
# Pearson - coeficiente usado para medir o grau de relacionamento entre duas variáveis com relação linear
# Spearman - teste não paramétrico, para medir o grau de relacionamento entre duas variaveis
# Kendall - teste não paramétrico, para medir a força de dependência entre duas variaveis
# Vetor com os métodos de correlação
metodos <- c("pearson", "spearman")
# Aplicando os métodos de correlação com a função cor()
cors <- lapply(metodos, function(method)
(cor(data, method = method)))
head(cors)
# Preprando o plot
require(lattice)
plot.cors <- function(x, labs){
diag(x) <- 0.0
plot( levelplot(x,
main = paste("Plot de Correlação usando Metodo", labs),
scales = list(x = list(rot = 90), cex = 1.0)) )
}
# Mapa de Correlação
Map(plot.cors, cors, metodos)
################ Correlação ##########################
# Métodos de Correlação
# Pearson - coeficiente usado para medir o grau de relacionamento entre duas variáveis com relação linear
# Spearman - teste não paramétrico, para medir o grau de relacionamento entre duas variaveis
# Kendall - teste não paramétrico, para medir a força de dependência entre duas variaveis
# Vetor com os métodos de correlação
metodos <- c("pearson", "spearman")
# Aplicando os métodos de correlação com a função cor()
cors <- lapply(metodos, function(method)
(cor(data, method = method)))
head(cors)
# Preprando o plot
require(lattice)
plot.cors <- function(x, labs){
diag(x) <- 0.0
plot( levelplot(x,
main = paste("Plot de Correlação usando Metodo", labs,"com outliers"),
scales = list(x = list(rot = 90), cex = 1.0)) )
}
# Mapa de Correlação
Map(plot.cors, cors, metodos)
library(ggplot2)
lapply(names(data), function(x){
ggplot(data, aes_string(x)) +
geom_histogram(bins = 30L, fill = "blue", alpha = 0.5) +
ggtitle(paste("Histograma de", x,"com outliers"))
})
for(v in names(data)){
upper_bound <- median(data[[v]]) + 3*mad(data[[v]])
lower_bound <- median(data[[v]]) - 3*mad(data[[v]])
data <- subset(data, data[[v]] <= upper_bound & data[[v]] >= lower_bound)
print(v)
print(nrow(data))
}
summary(data)
lapply(names(data), function(x){
ggplot(data, aes_string(x)) +
geom_histogram(bins = 100L, fill = "green", alpha = 0.5) +
ggtitle(paste("Histograma de", x,"sem outliers"))
})
# Métodos de Correlação
# Pearson - coeficiente usado para medir o grau de relacionamento entre duas variáveis com relação linear
# Spearman - teste não paramétrico, para medir o grau de relacionamento entre duas variaveis
# Kendall - teste não paramétrico, para medir a força de dependência entre duas variaveis
# Vetor com os métodos de correlação
metodos <- c("pearson", "spearman")
# Aplicando os métodos de correlação com a função cor()
cors <- lapply(metodos, function(method)
(cor(data, method = method)))
head(cors)
# Preprando o plot
plot.cors <- function(x, labs){
diag(x) <- 0.0
plot( levelplot(x,
main = paste("Plot de Correlação usando metodo", labs, "sem outliers"),
scales = list(x = list(rot = 90), cex = 1.0)) )
}
# Mapa de Correlação
Map(plot.cors, cors, metodos)
boxplot(data, Demanda_uni_equil ~ ., col = "blue", main = "Boxplot com os outliers")
names(data)
names(data[, -7])
boxplot(data[, -7], Demanda_uni_equil ~ ., col = "green", main = "Boxplot sem os outliers")
boxplot(data[, -1], Demanda_uni_equil ~ ., col = "green", main = "Boxplot sem os outliers")
